{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Ensemble Methods - Continuation\n",
    "\n",
    "\n",
    "by [Alejandro Correa Bahnsen](albahnsen.com/)\n",
    "\n",
    "version 0.2, May 2016\n",
    "\n",
    "## Part of the class [Machine Learning for Risk Management](https://github.com/albahnsen/ML_RiskManagement)\n",
    "\n",
    "\n",
    "This notebook is licensed under a [Creative Commons Attribution-ShareAlike 3.0 Unported License](http://creativecommons.org/licenses/by-sa/3.0/deed.en_US). Special thanks goes to [Kevin Markham](https://github.com/justmarkham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we learning about ensembling?\n",
    "\n",
    "- Very popular method for improving the predictive performance of machine learning models\n",
    "- Provides a foundation for understanding more sophisticated models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Combination of classifiers - Majority Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The most typical form of an ensemble is made by combining $T$ different base classifiers.\n",
    "  Each  base classifier $M(\\mathcal{S}_j)$ is trained by applying algorithm $M$ to a random subset \n",
    "  $\\mathcal{S}_j$ of the training set $\\mathcal{S}$.  \n",
    "  For simplicity we define $M_j \\equiv  M(\\mathcal{S}_j)$ for $j=1,\\dots,T$, and \n",
    "  $\\mathcal{M}=\\{M_j\\}_{j=1}^{T}$ a set of base classifiers.\n",
    "  Then, these models are combined using majority voting to create the ensemble $H$ as follows\n",
    "  $$\n",
    "    f_{mv}(\\mathcal{S},\\mathcal{M}) = max_{c \\in \\{0,1\\}} \\sum_{j=1}^T \n",
    "    \\mathbf{1}_c(M_j(\\mathcal{S})).\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in and prepare the chrun data\n",
    "# Download the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('../datasets/churn.csv')\n",
    "\n",
    "# Create X and y\n",
    "\n",
    "# Select only the numeric features\n",
    "X = data.iloc[:, [1,2,6,7,8,9,10]].astype(np.float)\n",
    "# Convert bools to floats\n",
    "X = X.join((data.iloc[:, [4,5]] == 'no').astype(np.float))\n",
    "\n",
    "y = (data.iloc[:, -1] == 'True.').astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account Length</th>\n",
       "      <th>Area Code</th>\n",
       "      <th>VMail Message</th>\n",
       "      <th>Day Mins</th>\n",
       "      <th>Day Calls</th>\n",
       "      <th>Day Charge</th>\n",
       "      <th>Eve Mins</th>\n",
       "      <th>Int'l Plan</th>\n",
       "      <th>VMail Plan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>45.07</td>\n",
       "      <td>197.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123.0</td>\n",
       "      <td>27.47</td>\n",
       "      <td>195.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114.0</td>\n",
       "      <td>41.38</td>\n",
       "      <td>121.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>299.4</td>\n",
       "      <td>71.0</td>\n",
       "      <td>50.90</td>\n",
       "      <td>61.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>113.0</td>\n",
       "      <td>28.34</td>\n",
       "      <td>148.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Account Length  Area Code  VMail Message  Day Mins  Day Calls  Day Charge  \\\n",
       "0           128.0      415.0           25.0     265.1      110.0       45.07   \n",
       "1           107.0      415.0           26.0     161.6      123.0       27.47   \n",
       "2           137.0      415.0            0.0     243.4      114.0       41.38   \n",
       "3            84.0      408.0            0.0     299.4       71.0       50.90   \n",
       "4            75.0      415.0            0.0     166.7      113.0       28.34   \n",
       "\n",
       "   Eve Mins  Int'l Plan  VMail Plan  \n",
       "0     197.4         1.0         0.0  \n",
       "1     195.5         1.0         0.0  \n",
       "2     121.2         1.0         1.0  \n",
       "3      61.9         0.0         1.0  \n",
       "4     148.3         0.0         1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2850</td>\n",
       "      <td>0.855086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>483</td>\n",
       "      <td>0.144914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  percentage\n",
       "0   2850    0.855086\n",
       "1    483    0.144914"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts().to_frame('count').assign(percentage = lambda x: x/x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 100 decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 100\n",
    "# set a seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "\n",
    "# create bootstrap samples (will be used to select rows from the DataFrame)\n",
    "samples = [np.random.choice(a=n_samples, size=n_samples, replace=True) for _ in range(n_estimators)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "np.random.seed(123) \n",
    "seeds = np.random.randint(1, 10000, size=n_estimators)\n",
    "\n",
    "trees = {}\n",
    "for i in range(n_estimators):\n",
    "    trees[i] = DecisionTreeClassifier(max_features=\"sqrt\", max_depth=None, random_state=seeds[i])\n",
    "    trees[i].fit(X_train.iloc[samples[i]], y_train.iloc[samples[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1   2   3   4   5   6   7   8   9  ...  90  91  92  93  94  95  96  \\\n",
       "438    0   0   0   0   0   0   0   0   0   0 ...   1   0   0   0   0   0   0   \n",
       "2674   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   0   \n",
       "1345   0   0   0   1   0   0   0   0   0   1 ...   0   0   0   1   1   0   0   \n",
       "1957   0   0   0   0   0   0   0   0   0   1 ...   1   0   1   0   0   0   0   \n",
       "2148   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   1   0   \n",
       "\n",
       "      97  98  99  \n",
       "438    0   0   0  \n",
       "2674   0   0   0  \n",
       "1345   1   1   0  \n",
       "1957   0   1   0  \n",
       "2148   0   1   0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict \n",
    "y_pred_df = pd.DataFrame(index=X_test.index, columns=list(range(n_estimators)))\n",
    "for i in range(n_estimators):\n",
    "    y_pred_df.iloc[:, i] = trees[i].predict(X_test)\n",
    "\n",
    "y_pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438      2\n",
       "2674     5\n",
       "1345    35\n",
       "1957    17\n",
       "2148     3\n",
       "3106     4\n",
       "1786    22\n",
       "321      6\n",
       "3082    10\n",
       "2240     5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_df.sum(axis=1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5245901639344264"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_pred_df.sum(axis=1) >= (n_estimators / 2)).astype(np.int)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8945454545454545"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, bootstrap=True,\n",
    "                        random_state=42, n_jobs=-1, oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.536, 0.8945454545454545)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test), metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Combination of classifiers - Weighted Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority voting approach gives the same weight to each classfier regardless of the performance of each one. Why not take into account the oob performance of each classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, in the traditional approach, a \n",
    "similar comparison of the votes of the base classifiers is made, but giving a weight $\\alpha_j$ \n",
    "to each classifier $M_j$ during the voting phase\n",
    "$$\n",
    "  f_{wv}(\\mathcal{S},\\mathcal{M}, \\alpha)\n",
    "  =\\max_{c \\in \\{0,1\\}} \\sum_{j=1}^T \\alpha_j \\mathbf{1}_c(M_j(\\mathcal{S})),\n",
    "$$\n",
    "where $\\alpha=\\{\\alpha_j\\}_{j=1}^T$.\n",
    "The calculation of $\\alpha_j$ is related to the performance of each classifier $M_j$.\n",
    "It is usually defined as the normalized misclassification error   $\\epsilon$ of the base \n",
    "classifier $M_j$  in the out of bag set   $\\mathcal{S}_j^{oob}=\\mathcal{S}-\\mathcal{S}_j$\n",
    "\\begin{equation}\n",
    "  \\alpha_j=\\frac{1-\\epsilon(M_j(\\mathcal{S}_j^{oob}))}{\\sum_{j_1=1}^T \n",
    "  1-\\epsilon(M_{j_1}(\\mathcal{S}_{j_1}^{oob}))}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select each oob sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_oob = []\n",
    "# show the \"out-of-bag\" observations for each sample\n",
    "for sample in samples:\n",
    "    samples_oob.append(sorted(set(range(n_samples)) - set(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the oob error of each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.zeros(n_estimators)\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    y_pred_ = trees[i].predict(X_train.iloc[samples_oob[i]])\n",
    "    errors[i] = 1 - metrics.accuracy_score(y_train.iloc[samples_oob[i]], y_pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'OOB error of each tree')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAEeCAYAAAANcYvwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2cf13cf978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.scatter(range(n_estimators), errors)\n",
    "plt.xlim([0, n_estimators])\n",
    "plt.title('OOB error of each tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = (1 - errors) / (1 - errors).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weighted_sum_1 = ((y_pred_df) * alpha).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438     0.019993\n",
       "2674    0.050009\n",
       "1345    0.350236\n",
       "1957    0.170230\n",
       "2148    0.030047\n",
       "3106    0.040100\n",
       "1786    0.219819\n",
       "321     0.059707\n",
       "3082    0.100178\n",
       "2240    0.050128\n",
       "1910    0.180194\n",
       "2124    0.190111\n",
       "2351    0.049877\n",
       "1736    0.950014\n",
       "879     0.039378\n",
       "785     0.219632\n",
       "2684    0.010104\n",
       "787     0.710568\n",
       "170     0.220390\n",
       "1720    0.020166\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_sum_1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5267489711934156, 0.8954545454545455)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (weighted_sum_1 >= 0.5).astype(np.int)\n",
    "\n",
    "metrics.f1_score(y_pred, y_test), metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Weighted voting with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.536, 0.8945454545454545)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, bootstrap=True,\n",
    "                        random_state=42, n_jobs=-1, oob_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test), metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.zeros(clf.n_estimators)\n",
    "y_pred_all_ = np.zeros((X_test.shape[0], clf.n_estimators))\n",
    "\n",
    "for i in range(clf.n_estimators):\n",
    "    oob_sample = ~clf.estimators_samples_[i]\n",
    "    y_pred_ = clf.estimators_[i].predict(X_train.values[oob_sample])\n",
    "    errors[i] = metrics.accuracy_score(y_pred_, y_train.values[oob_sample])\n",
    "    y_pred_all_[:, i] = clf.estimators_[i].predict(X_test)\n",
    "    \n",
    "alpha = (1 - errors) / (1 - errors).sum()\n",
    "y_pred = (np.sum(y_pred_all_ * alpha, axis=1) >= 0.5).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5533596837944664, 0.8972727272727272)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_pred, y_test), metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Combination of classifiers - Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The staking method consists in combining the different base classifiers by learning a \n",
    "second level algorithm on top of them. In this framework, once the base \n",
    "classifiers are constructed using the training set  $\\mathcal{S}$, a new set is constructed \n",
    "where the output of the base classifiers  are now considered as the features while keeping the \n",
    "class labels.\n",
    "\n",
    "Even though there is no restriction on which algorithm can be used as a second level learner, \n",
    "it is common to use a linear model, such as \n",
    "$$\n",
    "  f_s(\\mathcal{S},\\mathcal{M},\\beta) =\n",
    "  g \\left( \\sum_{j=1}^T \\beta_j M_j(\\mathcal{S}) \\right),\n",
    "$$\n",
    "where $\\beta=\\{\\beta_j\\}_{j=1}^T$, and $g(\\cdot)$ is the sign function \n",
    "$g(z)=sign(z)$ in the case of a linear regression or the sigmoid function, defined \n",
    "as $g(z)=1/(1+e^{-z})$, in the case of a logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first get a new training set consisting of the output of every classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_2 = pd.DataFrame(index=X_train.index, columns=list(range(n_estimators)))\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    X_train_2[i] = trees[i].predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1   2   3   4   5   6   7   8   9  ...  90  91  92  93  94  95  96  \\\n",
       "2360   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   0   \n",
       "1412   0   0   1   0   0   0   0   0   0   0 ...   1   0   0   0   0   0   0   \n",
       "1404   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   1   0   0   \n",
       "626    1   1   0   1   1   1   1   1   1   1 ...   1   1   1   1   1   1   1   \n",
       "347    0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   0   \n",
       "\n",
       "      97  98  99  \n",
       "2360   0   0   0  \n",
       "1412   0   0   0  \n",
       "1404   0   0   0  \n",
       "626    1   1   1  \n",
       "347    0   0   0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegressionCV()\n",
    "lr.fit(X_train_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10093102, 0.1042197 , 0.09431205, 0.09652843, 0.09709429,\n",
       "        0.09902616, 0.11100235, 0.09662288, 0.09340919, 0.09112994,\n",
       "        0.10012606, 0.09821902, 0.09383543, 0.09553507, 0.09147579,\n",
       "        0.09649564, 0.08965686, 0.09196857, 0.09684012, 0.09020758,\n",
       "        0.09839592, 0.09513808, 0.1044603 , 0.10028703, 0.09671603,\n",
       "        0.09725639, 0.10912207, 0.10590827, 0.10275491, 0.10275279,\n",
       "        0.10607316, 0.09803225, 0.10319411, 0.0926599 , 0.09702325,\n",
       "        0.09524124, 0.088848  , 0.09960894, 0.09053403, 0.09010282,\n",
       "        0.0990557 , 0.0987997 , 0.10538386, 0.09584352, 0.09633964,\n",
       "        0.09001206, 0.09181887, 0.08995095, 0.10130986, 0.10827168,\n",
       "        0.10064992, 0.09771002, 0.08922346, 0.10078438, 0.10173442,\n",
       "        0.1052274 , 0.09743252, 0.09597317, 0.08932798, 0.10033609,\n",
       "        0.10346122, 0.10145004, 0.09017084, 0.10348697, 0.09335995,\n",
       "        0.09795824, 0.10166729, 0.09306547, 0.09538575, 0.10997592,\n",
       "        0.09352845, 0.09860336, 0.1059772 , 0.09583408, 0.09823145,\n",
       "        0.09995048, 0.10224689, 0.10065135, 0.10208938, 0.11257989,\n",
       "        0.09956423, 0.11515946, 0.09798322, 0.10092449, 0.10150098,\n",
       "        0.10275192, 0.09180693, 0.0990442 , 0.10016612, 0.10145948,\n",
       "        0.09848122, 0.10322931, 0.09913907, 0.08925477, 0.09950337,\n",
       "        0.10277594, 0.09249331, 0.0954106 , 0.1053263 , 0.09849884]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(y_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5365853658536585, 0.8963636363636364)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_pred, y_test), metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5625000000000001, 0.8981818181818182)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_all_ = np.zeros((X_test.shape[0], clf.n_estimators))\n",
    "X_train_3 = np.zeros((X_train.shape[0], clf.n_estimators))\n",
    "\n",
    "for i in range(clf.n_estimators):\n",
    "\n",
    "    X_train_3[:, i] = clf.estimators_[i].predict(X_train)\n",
    "    y_pred_all_[:, i] = clf.estimators_[i].predict(X_test)\n",
    "    \n",
    "lr = LogisticRegressionCV()\n",
    "lr.fit(X_train_3, y_train)\n",
    "\n",
    "y_pred = lr.predict(y_pred_all_)\n",
    "metrics.f1_score(y_pred, y_test), metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vs using only one dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.44510385756676557, 0.83)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test), metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Boosting\n",
    "\n",
    "While boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are typically weighted in some way that is usually related to the weak learners' accuracy. After a weak learner is added, the data is reweighted: examples that are misclassified gain weight and examples that are classified correctly lose weight (some boosting algorithms actually decrease the weight of repeatedly misclassified examples, e.g., boost by majority and BrownBoost). Thus, future weak learners focus more on the examples that previous weak learners misclassified. (Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/OurMethodv81.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost\n",
    "\n",
    "AdaBoost (adaptive boosting) is an ensemble learning algorithm that can be used for classification or regression. Although AdaBoost is more resistant to overfitting than many machine learning algorithms, it is often sensitive to noisy data and outliers.\n",
    "\n",
    "AdaBoost is called adaptive because it uses multiple iterations to generate a single composite strong learner. AdaBoost creates the strong learner (a classifier that is well-correlated to the true classifier) by iteratively adding weak learners (a classifier that is only slightly correlated to the true classifier). During each round of training, a new weak learner is added to the ensemble and a weighting vector is adjusted to focus on examples that were misclassified in previous rounds. The result is a classifier that has higher accuracy than the weak learnersâ€™ classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm:\n",
    "\n",
    "* Initialize all weights ($w_i$) to 1 / n_samples\n",
    "* Train a classifier $h_t$ using weights\n",
    "* Estimate training error $e_t$\n",
    "* set $alpha_t = log\\left(\\frac{1-e_t}{e_t}\\right)$\n",
    "* Update weights \n",
    "$$w_i^{t+1} = w_i^{t}e^{\\left(\\alpha_t \\mathbf{I}\\left(y_i \\ne h_t(x_t)\\right)\\right)}$$\n",
    "* Repeat while $e_t<0.5$ and $t<T$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in and prepare the chrun data\n",
    "# Download the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('../datasets/churn.csv')\n",
    "\n",
    "# Create X and y\n",
    "\n",
    "# Select only the numeric features\n",
    "X = data.iloc[:, [1,2,6,7,8,9,10]].astype(np.float)\n",
    "# Convert bools to floats\n",
    "X = X.join((data.iloc[:, [4,5]] == 'no').astype(np.float))\n",
    "\n",
    "y = (data.iloc[:, -1] == 'True.').astype(np.int)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "n_samples = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators = 10\n",
    "weights = pd.DataFrame(index=X_train.index, columns=list(range(n_estimators)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = 0\n",
    "weights[t] = 1 / n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "trees = []\n",
    "trees.append(DecisionTreeClassifier(max_depth=1))\n",
    "trees[t].fit(X_train, y_train, sample_weight=weights[t].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13613972234661886"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ = trees[t].predict(X_train)\n",
    "error = []\n",
    "error.append(1 - metrics.accuracy_score(y_pred_, y_train))\n",
    "error[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8477293114995077"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = []\n",
    "alpha.append(np.log((1 - error[t]) / error[t]))\n",
    "alpha[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights[t + 1] = weights[t]\n",
    "filter_ = y_pred_ != y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.loc[filter_, t + 1] = weights.loc[filter_, t] * np.exp(alpha[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[t + 1] = weights[t + 1] / weights[t + 1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iteration 2 - n_estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1, n_estimators):\n",
    "    trees.append(DecisionTreeClassifier(max_depth=1))\n",
    "    trees[t].fit(X_train, y_train, sample_weight=weights[t].values)\n",
    "    y_pred_ = trees[t].predict(X_train)\n",
    "    error.append(1 - metrics.accuracy_score(y_pred_, y_train))\n",
    "    alpha.append(np.log((1 - error[t]) / error[t]))\n",
    "    weights[t + 1] = weights[t]\n",
    "    filter_ = y_pred_ != y_train\n",
    "    weights.loc[filter_, t + 1] = weights.loc[filter_, t] * np.exp(alpha[t])\n",
    "    weights[t + 1] = weights[t + 1] / weights[t + 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13613972234661886,\n",
       " 0.15629198387819077,\n",
       " 0.8437080161218092,\n",
       " 0.8437080161218092,\n",
       " 0.8437080161218092,\n",
       " 0.8437080161218092,\n",
       " 0.8437080161218092,\n",
       " 0.8437080161218092,\n",
       " 0.8437080161218092,\n",
       " 0.8437080161218092]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create classification\n",
    "\n",
    "Only classifiers when error < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_n_estimators = np.sum([x<0.5 for x in error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_all = np.zeros((X_test.shape[0], new_n_estimators))\n",
    "for t in range(new_n_estimators):\n",
    "    y_pred_all[:, t] = trees[t].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (np.sum(y_pred_all * alpha[:new_n_estimators], axis=1) >= 1).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5105105105105104, 0.8518181818181818)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_pred, y_test.values), metrics.accuracy_score(y_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier()\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.29107981220657275, 0.8627272727272727)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test.values), metrics.accuracy_score(y_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5289256198347108, 0.8963636363636364)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test.values), metrics.accuracy_score(y_pred, y_test.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
